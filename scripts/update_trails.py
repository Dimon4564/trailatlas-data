#!/usr/bin/env python3
import argparse
import json
import math
import re
import hashlib
import time
from copy import deepcopy
from datetime import datetime, timezone
from pathlib import Path
import xml.etree.ElementTree as ET
from typing import List, Dict, Tuple, Optional

try:
    from deep_translator import GoogleTranslator
    TRANSLATOR_AVAILABLE = True
except ImportError:
    TRANSLATOR_AVAILABLE = False
    print("Warning: deep-translator not available. Translations will be skipped.")

LANGS = ("ru", "ro", "uk", "en")
DIFFICULTIES = ["green", "blue", "red", "black"]

# Country folder name to countryId mapping (extensible)
COUNTRY_FOLDERS = {
    "gpx_romania": "ro",
    "gpx_germany": "de",
    "gpx_poland": "pl",
}

# Reverse mapping for efficiency
COUNTRY_CODE_TO_FOLDER = {v: k for k, v in COUNTRY_FOLDERS.items()}

# Trail type names in all languages
TRAIL_TYPE_NAMES = {
    "xc": {"ru": "XC Ñ‚Ñ€ÐµÐ¹Ð»", "ro": "Traseu XC", "uk": "XC Ñ‚Ñ€ÐµÐ¹Ð»", "en": "XC Trail"},
    "trail": {"ru": "Ð¢Ñ€ÐµÐ¹Ð»", "ro": "Traseu", "uk": "Ð¢Ñ€ÐµÐ¹Ð»", "en": "Trail"},
    "enduro": {"ru": "Ð­Ð½Ð´ÑƒÑ€Ð¾", "ro": "Enduro", "uk": "Ð•Ð½Ð´ÑƒÑ€Ð¾", "en": "Enduro"},
    "dh": {"ru": "Ð”Ð°ÑƒÐ½Ñ…Ð¸Ð»Ð»", "ro": "Downhill", "uk": "Ð”Ð°ÑƒÐ½Ñ…Ñ–Ð»", "en": "Downhill"},
    "flow": {"ru": "Ð¤Ð»Ð¾Ñƒ", "ro": "Flow", "uk": "Ð¤Ð»Ð¾Ñƒ", "en": "Flow"}
}

# Trail type descriptions in Russian
TRAIL_TYPE_DESCRIPTIONS = {
    "xc": {
        "green": "ÐšÑ€Ð¾ÑÑ-ÐºÐ°Ð½Ñ‚Ñ€Ð¸ Ñ‚Ñ€ÐµÐ¹Ð» Ñ Ð¿Ð»Ð°Ð²Ð½Ñ‹Ð¼Ð¸ Ð¿Ð¾Ð´ÑŠÐµÐ¼Ð°Ð¼Ð¸",
        "blue": "ÐšÑ€Ð¾ÑÑ-ÐºÐ°Ð½Ñ‚Ñ€Ð¸ Ñ‚Ñ€ÐµÐ¹Ð» Ñ ÑƒÐ¼ÐµÑ€ÐµÐ½Ð½Ñ‹Ð¼Ð¸ Ð¿Ð¾Ð´ÑŠÐµÐ¼Ð°Ð¼Ð¸",
        "red": "Ð¢ÐµÑ…Ð½Ð¸Ñ‡ÐµÑÐºÐ¸Ð¹ ÐºÑ€Ð¾ÑÑ-ÐºÐ°Ð½Ñ‚Ñ€Ð¸ Ñ‚Ñ€ÐµÐ¹Ð»",
        "black": "Ð­ÐºÑÑ‚Ñ€ÐµÐ¼Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ñ‚ÐµÑ…Ð½Ð¸Ñ‡ÐµÑÐºÐ¸Ð¹ Ñ‚Ñ€ÐµÐ¹Ð»"
    },
    "trail": {
        "green": "Ð›ÐµÑÐ½Ð°Ñ Ñ‚Ñ€Ð¾Ð¿Ð° Ñ Ð½ÐµÑÐ»Ð¾Ð¶Ð½Ñ‹Ð¼Ð¸ ÑƒÑ‡Ð°ÑÑ‚ÐºÐ°Ð¼Ð¸",
        "blue": "Ð¢Ñ€ÐµÐ¹Ð»Ð¾Ð²Ð°Ñ Ñ‚Ñ€Ð°ÑÑÐ° Ñ Ñ‚ÐµÑ…Ð½Ð¸Ñ‡ÐµÑÐºÐ¸Ð¼Ð¸ ÑÐµÐºÑ†Ð¸ÑÐ¼Ð¸",
        "red": "Ð¢ÐµÑ…Ð½Ð¸Ñ‡ÐµÑÐºÐ¸ ÑÐ»Ð¾Ð¶Ð½Ð°Ñ Ñ‚Ñ€ÐµÐ¹Ð»Ð¾Ð²Ð°Ñ Ñ‚Ñ€Ð°ÑÑÐ°",
        "black": "Ð­ÐºÑÑ‚Ñ€ÐµÐ¼Ð°Ð»ÑŒÐ½Ð°Ñ Ñ‚Ñ€ÐµÐ¹Ð»Ð¾Ð²Ð°Ñ Ñ‚Ñ€Ð°ÑÑÐ° Ñ Ð²Ñ‹ÑÐ¾ÐºÐ¾Ð¹ Ñ‚ÐµÑ…Ð½Ð¸Ñ‡ÐµÑÐºÐ¾Ð¹ ÑÐ»Ð¾Ð¶Ð½Ð¾ÑÑ‚ÑŒÑŽ"
    },
    "enduro": {
        "green": "Ð­Ð½Ð´ÑƒÑ€Ð¾ Ñ‚Ñ€Ð°ÑÑÐ° Ð½Ð°Ñ‡Ð°Ð»ÑŒÐ½Ð¾Ð³Ð¾ ÑƒÑ€Ð¾Ð²Ð½Ñ",
        "blue": "Ð­Ð½Ð´ÑƒÑ€Ð¾ Ñ‚Ñ€Ð°ÑÑÐ° Ñ Ñ‡ÐµÑ€ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸ÐµÐ¼ Ð¿Ð¾Ð´ÑŠÐµÐ¼Ð¾Ð² Ð¸ ÑÐ¿ÑƒÑÐºÐ¾Ð²",
        "red": "Ð­Ð½Ð´ÑƒÑ€Ð¾ Ñ‚Ñ€Ð°ÑÑÐ° Ñ ÐºÑ€ÑƒÑ‚Ñ‹Ð¼Ð¸ ÑÐ¿ÑƒÑÐºÐ°Ð¼Ð¸ Ð¸ Ñ‚ÐµÑ…Ð½Ð¸Ñ‡ÐµÑÐºÐ¸Ð¼Ð¸ ÑƒÑ‡Ð°ÑÑ‚ÐºÐ°Ð¼Ð¸",
        "black": "Ð­ÐºÑÑ‚Ñ€ÐµÐ¼Ð°Ð»ÑŒÐ½Ð°Ñ ÑÐ½Ð´ÑƒÑ€Ð¾ Ñ‚Ñ€Ð°ÑÑÐ° Ñ Ð¾Ñ‡ÐµÐ½ÑŒ ÐºÑ€ÑƒÑ‚Ñ‹Ð¼Ð¸ Ð¸ Ñ‚ÐµÑ…Ð½Ð¸Ñ‡ÐµÑÐºÐ¸Ð¼Ð¸ ÑÐµÐºÑ†Ð¸ÑÐ¼Ð¸"
    },
    "dh": {
        "green": "Ð”Ð°ÑƒÐ½Ñ…Ð¸Ð»Ð» Ñ‚Ñ€Ð°ÑÑÐ° Ð´Ð»Ñ Ð½Ð°Ñ‡Ð¸Ð½Ð°ÑŽÑ‰Ð¸Ñ…",
        "blue": "Ð”Ð°ÑƒÐ½Ñ…Ð¸Ð»Ð» Ñ‚Ñ€Ð°ÑÑÐ° ÑÑ€ÐµÐ´Ð½ÐµÐ³Ð¾ ÑƒÑ€Ð¾Ð²Ð½Ñ",
        "red": "Ð¢ÐµÑ…Ð½Ð¸Ñ‡ÐµÑÐºÐ¸Ð¹ Ð´Ð°ÑƒÐ½Ñ…Ð¸Ð»Ð» Ñ ÐºÑ€ÑƒÑ‚Ñ‹Ð¼Ð¸ ÑÐ¿ÑƒÑÐºÐ°Ð¼Ð¸",
        "black": "Ð­ÐºÑÑ‚Ñ€ÐµÐ¼Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ð´Ð°ÑƒÐ½Ñ…Ð¸Ð»Ð» Ñ Ð¾Ñ‡ÐµÐ½ÑŒ ÐºÑ€ÑƒÑ‚Ñ‹Ð¼Ð¸ Ð¸ Ð¾Ð¿Ð°ÑÐ½Ñ‹Ð¼Ð¸ ÑƒÑ‡Ð°ÑÑ‚ÐºÐ°Ð¼Ð¸"
    },
    "flow": {
        "green": "Ð¤Ð»Ð¾Ñƒ-Ñ‚Ñ€ÐµÐ¹Ð» Ñ Ð¿Ð»Ð°Ð²Ð½Ñ‹Ð¼Ð¸ Ð²Ð¸Ñ€Ð°Ð¶Ð°Ð¼Ð¸",
        "blue": "Ð¤Ð»Ð¾Ñƒ-Ñ‚Ñ€ÐµÐ¹Ð» Ñ Ð¿Ñ€Ñ‹Ð¶ÐºÐ°Ð¼Ð¸ Ð¸ Ð²Ð¸Ñ€Ð°Ð¶Ð°Ð¼Ð¸",
        "red": "Ð¢ÐµÑ…Ð½Ð¸Ñ‡ÐµÑÐºÐ¸Ð¹ Ñ„Ð»Ð¾Ñƒ-Ñ‚Ñ€ÐµÐ¹Ð» Ñ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ð¼Ð¸ Ð¿Ñ€Ñ‹Ð¶ÐºÐ°Ð¼Ð¸",
        "black": "Ð­ÐºÑÑ‚Ñ€ÐµÐ¼Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ñ„Ð»Ð¾Ñƒ-Ñ‚Ñ€ÐµÐ¹Ð» Ð´Ð»Ñ Ð¿Ñ€Ð¾Ñ„ÐµÑÑÐ¸Ð¾Ð½Ð°Ð»Ð¾Ð²"
    }
}

# Surface type descriptions in Russian
SURFACE_DESCRIPTIONS = {
    "dirt": "Ð³Ñ€ÑƒÐ½Ñ‚Ð¾Ð²Ð¾Ðµ Ð¿Ð¾ÐºÑ€Ñ‹Ñ‚Ð¸Ðµ",
    "gravel": "Ð³Ñ€Ð°Ð²Ð¸Ð¹Ð½Ð¾Ðµ Ð¿Ð¾ÐºÑ€Ñ‹Ñ‚Ð¸Ðµ",
    "paved": "Ð°ÑÑ„Ð°Ð»ÑŒÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð°Ñ Ð¿Ð¾Ð²ÐµÑ€Ñ…Ð½Ð¾ÑÑ‚ÑŒ",
    "ground": "ÐµÑÑ‚ÐµÑÑ‚Ð²ÐµÐ½Ð½Ð¾Ðµ Ð¿Ð¾ÐºÑ€Ñ‹Ñ‚Ð¸Ðµ"
}

# Difficulty recommendations in Russian
DIFFICULTY_DESCRIPTIONS = {
    "green": "ÐŸÐ¾Ð´Ñ…Ð¾Ð´Ð¸Ñ‚ Ð´Ð»Ñ Ð½Ð°Ñ‡Ð¸Ð½Ð°ÑŽÑ‰Ð¸Ñ… Ñ€Ð°Ð¹Ð´ÐµÑ€Ð¾Ð²",
    "blue": "Ð¢Ñ€ÐµÐ±ÑƒÐµÑ‚ Ð±Ð°Ð·Ð¾Ð²Ñ‹Ñ… Ð½Ð°Ð²Ñ‹ÐºÐ¾Ð² ÐºÐ°Ñ‚Ð°Ð½Ð¸Ñ Ð¸ Ñ…Ð¾Ñ€Ð¾ÑˆÐµÐ¹ Ñ„Ð¸Ð·Ð¸Ñ‡ÐµÑÐºÐ¾Ð¹ Ð¿Ð¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²ÐºÐ¸",
    "red": "Ð ÐµÐºÐ¾Ð¼ÐµÐ½Ð´ÑƒÐµÑ‚ÑÑ Ð´Ð»Ñ Ð¾Ð¿Ñ‹Ñ‚Ð½Ñ‹Ñ… Ñ€Ð°Ð¹Ð´ÐµÑ€Ð¾Ð² Ñ Ñ…Ð¾Ñ€Ð¾ÑˆÐµÐ¹ Ñ‚ÐµÑ…Ð½Ð¸Ñ‡ÐµÑÐºÐ¾Ð¹ Ð¿Ð¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²ÐºÐ¾Ð¹",
    "black": "Ð¢ÐžÐ›Ð¬ÐšÐž Ð´Ð»Ñ ÑÐºÑÐ¿ÐµÑ€Ñ‚Ð¾Ð²! Ð¢Ñ€ÐµÐ±ÑƒÐµÑ‚ Ð¾Ñ‚Ð»Ð¸Ñ‡Ð½Ð¾Ð¹ Ñ‚ÐµÑ…Ð½Ð¸ÐºÐ¸, Ð¾Ð¿Ñ‹Ñ‚Ð° Ð¸ ÑÐ¿ÐµÑ†Ð¸Ð°Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ð¾Ð±Ð¾Ñ€ÑƒÐ´Ð¾Ð²Ð°Ð½Ð¸Ñ"
}

# ------------------ helpers ------------------
def today_yyyy_mm_dd():
    return datetime.now(timezone.utc).strftime("%Y-%m-%d")

def safe_float(x):
    try:
        return float(x)
    except Exception:
        return None

def sanitize_filename(name: str) -> str:
    """Sanitize a string to be a valid filename."""
    # Remove or replace invalid characters
    name = re.sub(r'[<>:"/\\|?*]', '', name)
    name = re.sub(r'\s+', '_', name)
    name = name.lower().strip('_')
    return name if name else "unnamed"

def convert_route_to_track(rte_elem, ns: str):
    """
    Convert a <rte> element to a <trk> element.
    Routes contain <rtept> elements directly, tracks contain <trkseg> with <trkpt> elements.
    """
    def q(tag): return f"{{{ns}}}{tag}" if ns else tag
    
    # Create new track element
    trk = ET.Element(q("trk"))
    
    # Copy name if exists
    name_elem = rte_elem.find(q("name"))
    if name_elem is not None:
        new_name = ET.SubElement(trk, q("name"))
        new_name.text = name_elem.text
    
    # Copy other metadata (desc, cmt, etc.)
    for child in rte_elem:
        if child.tag in [q("name"), q("rtept")]:
            continue  # Skip name (already copied) and route points (will be converted)
        trk.append(deepcopy(child))
    
    # Create track segment
    trkseg = ET.SubElement(trk, q("trkseg"))
    
    # Convert route points to track points
    for rtept in rte_elem.findall(q("rtept")):
        trkpt = ET.SubElement(trkseg, q("trkpt"), rtept.attrib)
        # Copy all children (ele, time, etc.)
        for child in rtept:
            trkpt.append(deepcopy(child))
    
    return trk

def should_process_track(trk_elem, gpx_root, ns: str) -> Tuple[bool, str]:
    """
    Check if track should be processed.
    Returns (should_process, skip_reason)
    """
    def q(tag): return f"{{{ns}}}{tag}" if ns else tag
    
    # Create temp GPX to analyze
    temp_root = ET.Element(gpx_root.tag, gpx_root.attrib)
    temp_root.append(deepcopy(trk_elem))
    temp_tree = ET.ElementTree(temp_root)
    
    # Save to temp file
    import tempfile
    temp_path = None
    try:
        with tempfile.NamedTemporaryFile(mode='w', suffix='.gpx', delete=False) as f:
            temp_file_name = f.name
        # Write after closing the file to avoid issues on some platforms
        temp_path = Path(temp_file_name)
        temp_tree.write(temp_path, encoding='utf-8', xml_declaration=True)
        
        # Check point count
        pts = parse_gpx_points(temp_path, include_elevation=True)
        if len(pts) < 10:
            return (False, f"too few points ({len(pts)})")
        
        # Check length
        stats = calculate_elevation_stats(pts)
        length = stats.get("total_distance", 0)
        
        if length < 900:
            return (False, f"too short ({length:.0f}m)")
        
        if length > 50000:
            return (False, f"too long ({length/1000:.1f}km)")
        
        return (True, "")
    
    finally:
        if temp_path is not None and temp_path.exists():
            temp_path.unlink()

def split_multi_track_gpx(gpx_path: Path, country_code: str) -> List[Path]:
    """
    Split a GPX file with multiple <trk> or <rte> elements into individual GPX files.
    Routes are automatically converted to tracks in output.

    IMPORTANT: Only processes tracks/routes that have a valid <name> tag.
    Tracks without names are skipped as they are usually low-quality data.

    Returns list of created file paths.
    """
    try:
        tree = ET.parse(gpx_path)
        root = tree.getroot()
        
        # Extract namespace
        m = re.match(r"\{(.+)\}", root.tag)
        ns = m.group(1) if m else ""
        def q(tag): return f"{{{ns}}}{tag}" if ns else tag
        
        tracks = root.findall(q("trk"))
        routes = root.findall(q("rte"))
        
        # Check if we need to split
        total_elements = len(tracks) + len(routes)
        if total_elements <= 1:
            # Single track/route or no tracks/routes, no need to split
            return []
        
        element_type = "tracks" if tracks else "routes"
        if tracks and routes:
            element_type = f"{len(tracks)} tracks and {len(routes)} routes"
        else:
            element_type = f"{total_elements} {element_type}"
        
        print(f"Splitting {gpx_path.name}: found {element_type}")
        
        created_files = []
        parent_dir = gpx_path.parent
        skipped_no_name = 0
        skipped_quality = 0
        
        # Process tracks
        for idx, trk in enumerate(tracks, 1):
            # Try to get track name - REQUIRED!
            name_elem = trk.find(q("name"))

            # Skip if no name or empty name
            if name_elem is None or not name_elem.text or not name_elem.text.strip():
                skipped_no_name += 1
                print(f"  âš ï¸  Skipping track {idx}: no name (likely garbage data)")
                continue

            track_name = name_elem.text.strip()

            # Check if track should be processed (quality checks)
            should_process, skip_reason = should_process_track(trk, root, ns)
            if not should_process:
                skipped_quality += 1
                print(f"  âš ï¸  Skipping '{track_name}': {skip_reason}")
                continue
            
            sanitized_name = sanitize_filename(track_name)
            new_filename = f"{sanitized_name}.gpx"
            
            # Ensure unique filename
            new_path = parent_dir / new_filename
            counter = 1
            while new_path.exists():
                stem = new_filename.rsplit('.', 1)[0]
                new_path = parent_dir / f"{stem}_{counter}.gpx"
                counter += 1
            
            # Create new GPX with single track
            new_root = ET.Element(root.tag, root.attrib)
            
            # Copy metadata if exists
            metadata = root.find(q("metadata"))
            if metadata is not None:
                new_root.append(deepcopy(metadata))
            
            # Add the single track
            new_root.append(trk)
            
            # Write to file
            new_tree = ET.ElementTree(new_root)
            ET.indent(new_tree, space="  ")
            new_tree.write(new_path, encoding="utf-8", xml_declaration=True)
            
            created_files.append(new_path)
            print(f"  âœ… Created: {new_path.name} ({track_name})")
        
        # Process routes (convert to tracks)
        for idx, rte in enumerate(routes, len(tracks) + 1):
            # Try to get route name - REQUIRED!
            name_elem = rte.find(q("name"))

            # Skip if no name or empty name
            if name_elem is None or not name_elem.text or not name_elem.text.strip():
                skipped_no_name += 1
                print(f"  âš ï¸  Skipping route {idx}: no name (likely garbage data)")
                continue

            route_name = name_elem.text.strip()

            # Convert route to track first for quality checking
            trk = convert_route_to_track(rte, ns)
            
            # Check if track should be processed (quality checks)
            should_process, skip_reason = should_process_track(trk, root, ns)
            if not should_process:
                skipped_quality += 1
                print(f"  âš ï¸  Skipping route '{route_name}': {skip_reason}")
                continue
            
            sanitized_name = sanitize_filename(route_name)
            new_filename = f"{sanitized_name}.gpx"
            
            # Ensure unique filename
            new_path = parent_dir / new_filename
            counter = 1
            while new_path.exists():
                stem = new_filename.rsplit('.', 1)[0]
                new_path = parent_dir / f"{stem}_{counter}.gpx"
                counter += 1
            
            # Create new GPX with route converted to track
            new_root = ET.Element(root.tag, root.attrib)
            
            # Copy metadata if exists
            metadata = root.find(q("metadata"))
            if metadata is not None:
                new_root.append(deepcopy(metadata))
            
            # Add the converted track (already done above)
            new_root.append(trk)
            
            # Write to file
            new_tree = ET.ElementTree(new_root)
            ET.indent(new_tree, space="  ")
            new_tree.write(new_path, encoding="utf-8", xml_declaration=True)
            
            created_files.append(new_path)
            print(f"  âœ… Created: {new_path.name} ({route_name} [route])")
        
        # Print summary
        print(f"\n  ðŸ“Š Summary for {gpx_path.name}:")
        print(f"     Created: {len(created_files)} trails")
        print(f"     Skipped (no name): {skipped_no_name}")
        print(f"     Skipped (quality): {skipped_quality}")

        # Delete original multi-track file
        gpx_path.unlink()
        print(f"  ðŸ—‘ï¸  Deleted original: {gpx_path.name}")
        
        return created_files
        
    except Exception as e:
        print(f"âŒ Error splitting {gpx_path}: {e}")
        return []

def calculate_elevation_stats(points: List[Tuple[float, float, Optional[float]]]) -> Dict[str, float]:
    """
    Calculate elevation statistics from points with elevation data.
    Returns dict with elevation_gain, elevation_loss, avg_gradient, total_distance, etc.
    Always calculates total_distance even without elevation data.
    """
    if not points:
        return {"elevation_gain": 0, "elevation_loss": 0, "avg_gradient": 0, "total_distance": 0, "has_elevation": False}
    
    # Filter points with elevation data
    points_with_ele = [(lat, lon, ele) for lat, lon, ele in points if ele is not None]
    
    # Calculate total distance using all points (even without elevation)
    total_distance = 0
    for i in range(1, len(points)):
        prev_lat, prev_lon = points[i-1][0], points[i-1][1]
        curr_lat, curr_lon = points[i][0], points[i][1]
        
        # Calculate distance (Haversine formula)
        lat1, lon1 = math.radians(prev_lat), math.radians(prev_lon)
        lat2, lon2 = math.radians(curr_lat), math.radians(curr_lon)
        dlat = lat2 - lat1
        dlon = lon2 - lon1
        a = math.sin(dlat/2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon/2)**2
        c = 2 * math.asin(math.sqrt(a))
        distance = 6371000 * c  # Earth radius in meters
        
        total_distance += distance
    
    # If we have no elevation data, return early with distance only
    if len(points_with_ele) < 2:
        return {"elevation_gain": 0, "elevation_loss": 0, "avg_gradient": 0, "total_distance": total_distance, "has_elevation": False}
    
    elevation_gain = 0
    elevation_loss = 0
    
    for i in range(1, len(points_with_ele)):
        prev_ele = points_with_ele[i-1][2]
        curr_ele = points_with_ele[i][2]
        
        # Calculate elevation change
        ele_change = curr_ele - prev_ele
        if ele_change > 0:
            elevation_gain += ele_change
        else:
            elevation_loss += abs(ele_change)
    
    # Calculate average gradient
    avg_gradient = 0
    if total_distance > 0:
        net_elevation = elevation_gain - elevation_loss
        avg_gradient = (net_elevation / total_distance) * 100  # percentage
    
    return {
        "elevation_gain": elevation_gain,
        "elevation_loss": elevation_loss,
        "avg_gradient": abs(avg_gradient),
        "total_distance": total_distance,
        "has_elevation": True
    }

def determine_difficulty_from_elevation(stats: Dict[str, float], trail_id: str) -> str:
    """
    Determine difficulty based on elevation statistics.
    Falls back to stable random if no elevation data.
    """
    if not stats.get("has_elevation", False):
        return stable_random_difficulty(trail_id)
    
    avg_gradient = stats.get("avg_gradient", 0)
    
    if avg_gradient < 5:
        return "green"
    elif avg_gradient < 10:
        return "blue"
    elif avg_gradient < 15:
        return "red"
    else:
        return "black"

def detect_trail_styles(stats: Dict[str, float], points: List) -> List[str]:
    """Auto-detect trail styles based on characteristics."""
    styles = []
    
    if not stats.get("has_elevation", False):
        return ["MTB"]
    
    elevation_loss = stats.get("elevation_loss", 0)
    elevation_gain = stats.get("elevation_gain", 0)
    
    # Check if it's primarily downhill
    if elevation_loss > elevation_gain * 2 and elevation_loss > 50:
        styles = ["MTB", "DH"]
    else:
        styles = ["MTB"]
    
    return styles

def generate_suitable_text(difficulty: str, styles: List[str]) -> str:
    """Generate suitable field based on difficulty and styles."""
    # Convert styles list to tuple for dictionary key
    styles_tuple = tuple(sorted(styles)) if styles else ()
    
    style_map = {
        ("green", ("MTB",)): "Cross Country / All Mountain",
        ("blue", ("MTB",)): "All Mountain / Cross Country",
        ("blue", ("DH", "MTB")): "All Mountain / Enduro / Downhill",
        ("red", ("MTB",)): "All Mountain / Enduro",
        ("red", ("DH", "MTB")): "Enduro / Downhill",
        ("black", ("DH", "MTB")): "Downhill / Freeride",
        ("black", ("DH",)): "Downhill / Freeride"
    }
    
    key = (difficulty, styles_tuple)
    if key in style_map:
        return style_map[key]
    
    # Default based on difficulty
    if difficulty == "green":
        return "Cross Country / All Mountain"
    elif difficulty == "blue":
        return "All Mountain / Cross Country"
    elif difficulty == "red":
        return "All Mountain / Enduro"
    else:
        return "Downhill / Freeride"

def generate_description(gpx_path: Path, stats: Dict[str, float], difficulty: str, 
                        country_code: str, trail_type: str) -> str:
    """Generate detailed, unique description based on trail characteristics"""
    distance_km = stats.get("total_distance", 0) / 1000
    elevation_gain = stats.get("elevation_gain", 0)
    elevation_loss = stats.get("elevation_loss", 0)
    
    # Extract OSM surface info if available
    surface = extract_surface_type(gpx_path) if gpx_path else None
    
    # Build description parts
    parts = []
    
    # 1. Basic length and elevation
    if stats.get("has_elevation", False):
        if elevation_loss > elevation_gain * 1.2:
            parts.append(f"Ð¢Ñ€ÐµÐ¹Ð» Ð¿Ñ€Ð¾Ñ‚ÑÐ¶ÐµÐ½Ð½Ð¾ÑÑ‚ÑŒÑŽ {distance_km:.1f} ÐºÐ¼ Ñ Ð¿ÐµÑ€ÐµÐ¿Ð°Ð´Ð¾Ð¼ Ð²Ñ‹ÑÐ¾Ñ‚Ñ‹ {elevation_loss:.0f} Ð¼")
        else:
            parts.append(f"Ð¢Ñ€ÐµÐ¹Ð» Ð¿Ñ€Ð¾Ñ‚ÑÐ¶ÐµÐ½Ð½Ð¾ÑÑ‚ÑŒÑŽ {distance_km:.1f} ÐºÐ¼ Ñ Ð½Ð°Ð±Ð¾Ñ€Ð¾Ð¼ Ð²Ñ‹ÑÐ¾Ñ‚Ñ‹ {elevation_gain:.0f} Ð¼")
    else:
        parts.append(f"Ð¢Ñ€ÐµÐ¹Ð» Ð¿Ñ€Ð¾Ñ‚ÑÐ¶ÐµÐ½Ð½Ð¾ÑÑ‚ÑŒÑŽ {distance_km:.1f} ÐºÐ¼")
    
    # 2. Trail type description - MATCH with difficulty properly
    if trail_type in TRAIL_TYPE_DESCRIPTIONS and difficulty in TRAIL_TYPE_DESCRIPTIONS[trail_type]:
        parts.append(TRAIL_TYPE_DESCRIPTIONS[trail_type][difficulty])
    else:
        # Fallback generic descriptions
        parts.append("Ð“Ð¾Ñ€Ð½Ñ‹Ð¹ Ð²ÐµÐ»Ð¾ÑÐ¸Ð¿ÐµÐ´Ð½Ñ‹Ð¹ Ñ‚Ñ€ÐµÐ¹Ð»")
    
    # 3. Surface type if available
    if surface and surface in SURFACE_DESCRIPTIONS:
        parts.append(f"ÐŸÐ¾ÐºÑ€Ñ‹Ñ‚Ð¸Ðµ: {SURFACE_DESCRIPTIONS[surface]}")
    
    # 4. Difficulty recommendation
    if difficulty in DIFFICULTY_DESCRIPTIONS:
        parts.append(DIFFICULTY_DESCRIPTIONS[difficulty])
    
    # Join parts
    desc = ". ".join(parts) + "."
    
    return desc

def stable_random_difficulty(trail_id: str) -> str:
    # "Ñ€Ð°Ð½Ð´Ð¾Ð¼", Ð½Ð¾ ÑÑ‚Ð°Ð±Ð¸Ð»ÑŒÐ½Ñ‹Ð¹: Ð¾Ð´Ð¸Ð½ Ð¸ Ñ‚Ð¾Ñ‚ Ð¶Ðµ id -> Ð¾Ð´Ð½Ð° Ð¸ Ñ‚Ð° Ð¶Ðµ ÑÐ»Ð¾Ð¶Ð½Ð¾ÑÑ‚ÑŒ
    h = hashlib.md5(trail_id.encode("utf-8")).hexdigest()
    n = int(h[:8], 16)
    return DIFFICULTIES[n % len(DIFFICULTIES)]

def parse_gpx_points(gpx_path: Path, include_elevation: bool = True):
    """
    Returns list of (lat, lon, ele) tuples.
    Supports GPX 1.0/1.1 namespaces.
    If include_elevation is False, returns (lat, lon) tuples for backward compatibility.
    """
    tree = ET.parse(gpx_path)
    root = tree.getroot()
    m = re.match(r"\{(.+)\}", root.tag)
    ns = m.group(1) if m else ""
    def q(tag): return f"{{{ns}}}{tag}" if ns else tag

    pts = []
    # trkpt
    for trk in root.findall(q("trk")):
        for seg in trk.findall(f".//{q('trkseg')}"):
            for pt in seg.findall(q("trkpt")):
                lat = safe_float(pt.attrib.get("lat"))
                lon = safe_float(pt.attrib.get("lon"))
                if lat is None or lon is None:
                    continue
                
                if include_elevation:
                    ele_elem = pt.find(q("ele"))
                    ele = safe_float(ele_elem.text) if ele_elem is not None else None
                    pts.append((lat, lon, ele))
                else:
                    pts.append((lat, lon))

    # fallback rtept
    if not pts:
        for rte in root.findall(q("rte")):
            for pt in rte.findall(q("rtept")):
                lat = safe_float(pt.attrib.get("lat"))
                lon = safe_float(pt.attrib.get("lon"))
                if lat is None or lon is None:
                    continue
                
                if include_elevation:
                    ele_elem = pt.find(q("ele"))
                    ele = safe_float(ele_elem.text) if ele_elem is not None else None
                    pts.append((lat, lon, ele))
                else:
                    pts.append((lat, lon))

    return pts

def get_gpx_track_name(gpx_path: Path) -> Optional[str]:
    """Extract track or route name from GPX file."""
    try:
        tree = ET.parse(gpx_path)
        root = tree.getroot()
        m = re.match(r"\{(.+)\}", root.tag)
        ns = m.group(1) if m else ""
        def q(tag): return f"{{{ns}}}{tag}" if ns else tag
        
        # Try track name first
        for trk in root.findall(q("trk")):
            name_elem = trk.find(q("name"))
            if name_elem is not None and name_elem.text:
                return name_elem.text
        
        # Try route name
        for rte in root.findall(q("rte")):
            name_elem = rte.find(q("name"))
            if name_elem is not None and name_elem.text:
                return name_elem.text
        
        # Try metadata name
        metadata = root.find(q("metadata"))
        if metadata is not None:
            name_elem = metadata.find(q("name"))
            if name_elem is not None and name_elem.text:
                return name_elem.text
        
        return None
    except Exception:
        return None

def extract_osm_name(gpx_path: Path) -> Optional[str]:
    """Extract name from OSM extensions (ogr:name or ogr:ref)"""
    try:
        tree = ET.parse(gpx_path)
        root = tree.getroot()
        m = re.match(r"\{(.+)\}", root.tag)
        ns = m.group(1) if m else ""
        
        # Check for ogr:ref (e.g., "DC49")
        for ext in root.findall('.//{*}extensions'):
            ref_elem = ext.find('.//{*}ref')
            if ref_elem is not None and ref_elem.text:
                return f"Route {ref_elem.text}"
            
            name_elem = ext.find('.//{*}name')
            if name_elem is not None and name_elem.text:
                return name_elem.text
        
        return None
    except Exception:
        return None

def detect_region_from_coords(lat: float, lon: float) -> str:
    """Simple region detection for Romania based on coordinates"""
    # Approximate regions based on lat/lon ranges
    if 45.4 <= lat <= 45.9 and 21.2 <= lon <= 21.8:
        return "TimiÈ™"
    elif 44.7 <= lat <= 45.2 and 22.5 <= lon <= 23.0:
        return "CaraÈ™-Severin"
    elif 47.0 <= lat <= 47.8 and 23.5 <= lon <= 24.5:
        return "Cluj"
    elif 46.0 <= lat <= 46.5 and 24.5 <= lon <= 25.5:
        return "BraÈ™ov"
    elif 45.0 <= lat <= 45.5 and 25.5 <= lon <= 26.5:
        return "Prahova"
    # Add more regions as needed
    return "Romania"

def generate_smart_name(gpx_path: Path, stats: Dict, trail_type: str) -> Optional[Dict[str, str]]:
    """Generate smart name based on location and trail characteristics"""
    try:
        # Get start coordinates
        pts = parse_gpx_points(gpx_path, include_elevation=False)
        if not pts:
            return None
        
        start_lat, start_lon = pts[0]
        
        # Simple location-based naming (without external API to avoid rate limits)
        region = detect_region_from_coords(start_lat, start_lon)
        
        length_km = stats.get("total_distance", 0) / 1000
        
        # Get trail type names from constants
        type_name = TRAIL_TYPE_NAMES.get(trail_type, TRAIL_TYPE_NAMES["trail"])
        
        return {
            "ru": f"{type_name['ru']} {region} {length_km:.1f} ÐºÐ¼",
            "ro": f"{type_name['ro']} {region} {length_km:.1f} km",
            "uk": f"{type_name['uk']} {region} {length_km:.1f} ÐºÐ¼",
            "en": f"{type_name['en']} {region} {length_km:.1f} km"
        }
    except Exception:
        return None

def determine_trail_type(gpx_path: Path, stats: Dict[str, float], difficulty: str = "green") -> str:
    """
    Determine trail type from OSM tags, characteristics, AND difficulty.
    Black/Red trails should never be classified as XC.
    """
    try:
        tree = ET.parse(gpx_path)
        root = tree.getroot()
        
        # Extract OSM tags from extensions
        osm_tags = {}
        for ext in root.findall('.//{*}extensions'):
            for child in ext:
                # Extract tag name (remove namespace)
                tag_name = child.tag.split('}')[-1] if '}' in child.tag else child.tag
                if child.text:
                    osm_tags[tag_name] = child.text
        
        # Check mtb:scale (most reliable)
        if 'mtb:scale' in osm_tags:
            try:
                scale = int(osm_tags['mtb:scale'])
                if scale >= 4:
                    return "dh"
                elif scale >= 2:
                    return "enduro"
                elif scale >= 1:
                    return "trail"
                else:
                    return "xc"
            except ValueError:
                pass
        
        # Check highway type
        if 'highway' in osm_tags:
            highway = osm_tags['highway']
            if highway == 'path':
                # Paths with difficulty should not be XC
                if difficulty in ["black", "red"]:
                    return "enduro"
                elif difficulty == "blue":
                    return "trail"
        
        # Fallback to gradient-based detection
        if not stats.get("has_elevation", False):
            # No elevation data, but check difficulty
            if difficulty == "black":
                return "dh"  # Black without elevation = assume DH
            elif difficulty == "red":
                return "enduro"
            else:
                return "trail"
        
        avg_gradient = stats.get("avg_gradient", 0)
        elevation_loss = stats.get("elevation_loss", 0)
        elevation_gain = stats.get("elevation_gain", 0)
        
        # Priority 1: Check if primarily downhill (DH indicator)
        if elevation_loss > elevation_gain * 1.5 and elevation_loss > 100:
            return "dh"
        
        # Priority 2: Difficulty-based classification
        # Black difficulty should NEVER be XC
        if difficulty == "black":
            if avg_gradient > 15 or elevation_loss > 200:
                return "dh"
            else:
                return "enduro"
        
        # Red difficulty should be at least trail/enduro
        if difficulty == "red":
            if avg_gradient > 12:
                return "enduro"
            else:
                return "trail"
        
        # Blue/Green can be XC or trail
        if avg_gradient > 8:
            return "trail"
        elif avg_gradient > 5:
            return "trail"
        else:
            return "xc"
    except Exception:
        return "xc"

def extract_surface_type(gpx_path: Path) -> Optional[str]:
    """Extract surface type from OSM tags"""
    try:
        tree = ET.parse(gpx_path)
        root = tree.getroot()
        
        for ext in root.findall('.//{*}extensions'):
            surface_elem = ext.find('.//{*}surface')
            if surface_elem is not None and surface_elem.text:
                return surface_elem.text
        
        return None
    except Exception:
        return None

def scan_gpx_files(gpx_dir: Path):
    if not gpx_dir.exists():
        return []
    return sorted([p for p in gpx_dir.rglob("*.gpx") if p.is_file()])

def preprocess_split_multi_track_gpx(gpx_dir: Path):
    """
    Scan all country subfolders for multi-track GPX files and split them.
    """
    if not gpx_dir.exists():
        return
    
    # Check country subfolders
    for folder_name, country_code in COUNTRY_FOLDERS.items():
        folder_path = gpx_dir / folder_name
        if not folder_path.exists():
            continue
        
        gpx_files = sorted([p for p in folder_path.glob("*.gpx") if p.is_file()])
        
        for gpx_file in gpx_files:
            split_multi_track_gpx(gpx_file, country_code)

def load_root(path: Path):
    if not path.exists():
        return None
    with path.open("r", encoding="utf-8") as f:
        return json.load(f)

def save_root(path: Path, root_obj):
    with path.open("w", encoding="utf-8") as f:
        json.dump(root_obj, f, ensure_ascii=False, indent=2)
        f.write("\n")

def translate_text(text: str, target_lang: str) -> str:
    """Translate text to target language using deep-translator."""
    if not TRANSLATOR_AVAILABLE or not text or not text.strip():
        return text
    
    try:
        # Map language codes to deep-translator format
        lang_map = {
            "ru": "ru",
            "ro": "ro",
            "uk": "uk",
            "en": "en"
        }
        
        target = lang_map.get(target_lang, target_lang)
        translator = GoogleTranslator(source='auto', target=target)
        result = translator.translate(text)
        time.sleep(0.05)  # Reduced delay for better performance
        return result if result else text
    except Exception as e:
        # Silently fail and return original text
        return text

def auto_translate_i18n(source_text: str, source_lang: str = "ru") -> Dict[str, str]:
    """Auto-translate text to all languages."""
    result = {}
    for lang in LANGS:
        if lang == source_lang:
            result[lang] = source_text
        else:
            translated = translate_text(source_text, lang)
            # If translation is available and successful, use it; otherwise use source
            result[lang] = translated if TRANSLATOR_AVAILABLE and translated else source_text
    return result

def default_i18n(text: str):
    return {k: text for k in LANGS}

def normalize_i18n(d, fallback: str):
    if isinstance(d, dict):
        out = {}
        for k in LANGS:
            out[k] = d.get(k, fallback)
        return out
    return default_i18n(fallback)

def roots_equal(a, b):
    # Compare content ignoring formatting/order noise
    return json.dumps(a, ensure_ascii=False, sort_keys=True) == json.dumps(b, ensure_ascii=False, sort_keys=True)

# ------------------ main trail builder ------------------
def build_trail_object(prev: dict, tid: str, gpx_url: str, start_lat, start_lon, 
                       gpx_path: Path = None, country_id: str = None, 
                       is_unverified: bool = False):
    """
    Build object exactly in your schema:
    id, cityId, styles, name, suitable, difficulty, desc, gpxUrl, startLat, startLon
    (difficulty stable-random if missing/invalid)
    cityId forced to chisinau
    
    For unverified trails from country folders, auto-generate fields.
    """
    # Force cityId for every trail
    city_id = prev.get("cityId", "chisinau")

    # Get elevation stats if GPX path provided
    stats = {"has_elevation": False}
    pts_with_ele = []
    if gpx_path and gpx_path.exists():
        pts_with_ele = parse_gpx_points(gpx_path, include_elevation=True)
        if pts_with_ele:
            stats = calculate_elevation_stats(pts_with_ele)
    
    # Handle difficulty
    prev_diff = prev.get("difficulty", "")
    if isinstance(prev_diff, str) and prev_diff.strip() in DIFFICULTIES:
        difficulty = prev_diff.strip()
    elif is_unverified and stats.get("has_elevation", False):
        # Auto-determine difficulty for unverified trails
        difficulty = determine_difficulty_from_elevation(stats, tid)
    else:
        difficulty = stable_random_difficulty(tid)

    # Handle styles
    prev_styles = prev.get("styles", [])
    if isinstance(prev_styles, list) and len(prev_styles) > 0:
        styles = prev_styles
    elif is_unverified and stats.get("has_elevation", False):
        # Auto-detect styles for unverified trails
        styles = detect_trail_styles(stats, pts_with_ele)
    else:
        styles = []

    # Handle name - smart generation with priority order
    prev_name = prev.get("name")
    if isinstance(prev_name, dict):
        # Check if we have any existing values
        has_existing = any(v for v in prev_name.values() if v)
        if has_existing:
            # Normalize to include all languages
            name = {}
            for lang in LANGS:
                if lang in prev_name and prev_name[lang]:
                    name[lang] = prev_name[lang]
                else:
                    # Fill missing language with Russian or first available
                    name[lang] = prev_name.get("ru") or next((v for v in prev_name.values() if v), tid)
        else:
            # No existing values, try smart generation
            # Priority 1: Try GPX <name> tag
            gpx_name = None
            if gpx_path and gpx_path.exists():
                gpx_name = get_gpx_track_name(gpx_path)
            
            if gpx_name:
                name = auto_translate_i18n(gpx_name, "ru")
            else:
                # Priority 2: Try OSM ref
                osm_name = extract_osm_name(gpx_path) if gpx_path else None
                
                if osm_name:
                    name = auto_translate_i18n(osm_name, "en")
                elif is_unverified and stats.get("total_distance", 0) > 0:
                    # Priority 3: Generate smart name from geolocation + characteristics
                    trail_type = determine_trail_type(gpx_path, stats, difficulty) if gpx_path else "xc"
                    smart_name = generate_smart_name(gpx_path, stats, trail_type)
                    
                    if smart_name:
                        name = smart_name
                    else:
                        # Priority 4: Fallback to cleaned filename
                        cleaned = tid.replace("_", " ").title()
                        name = auto_translate_i18n(f"{cleaned} Trail", "en")
                else:
                    # Fallback
                    cleaned = tid.replace("_", " ").title()
                    name = default_i18n(cleaned)
    else:
        # Try smart generation
        # Priority 1: Try GPX <name> tag
        gpx_name = None
        if gpx_path and gpx_path.exists():
            gpx_name = get_gpx_track_name(gpx_path)
        
        if gpx_name:
            name = auto_translate_i18n(gpx_name, "ru")
        else:
            # Priority 2: Try OSM ref
            osm_name = extract_osm_name(gpx_path) if gpx_path else None
            
            if osm_name:
                name = auto_translate_i18n(osm_name, "en")
            elif is_unverified and stats.get("total_distance", 0) > 0:
                # Priority 3: Generate smart name from geolocation + characteristics
                trail_type = determine_trail_type(gpx_path, stats, difficulty) if gpx_path else "xc"
                smart_name = generate_smart_name(gpx_path, stats, trail_type)
                
                if smart_name:
                    name = smart_name
                else:
                    # Priority 4: Fallback to cleaned filename
                    cleaned = tid.replace("_", " ").title()
                    name = auto_translate_i18n(f"{cleaned} Trail", "en")
            else:
                # Fallback
                name = default_i18n(tid)
    
    # Handle suitable field
    prev_suitable = prev.get("suitable")
    if isinstance(prev_suitable, dict):
        has_existing = any(v for v in prev_suitable.values() if v)
        if has_existing:
            suitable = {}
            for lang in LANGS:
                if lang in prev_suitable and prev_suitable[lang]:
                    suitable[lang] = prev_suitable[lang]
                else:
                    # Fill missing language with Russian or first available
                    suitable[lang] = prev_suitable.get("ru") or next((v for v in prev_suitable.values() if v), "")
        elif is_unverified:
            # Auto-generate suitable for unverified trails
            suitable_text = generate_suitable_text(difficulty, styles)
            suitable = auto_translate_i18n(suitable_text, "en")
        else:
            suitable = default_i18n("")
    elif is_unverified:
        # Auto-generate suitable for unverified trails
        suitable_text = generate_suitable_text(difficulty, styles)
        suitable = auto_translate_i18n(suitable_text, "en")
    else:
        suitable = default_i18n("")
    
    # Handle description - enhanced generation
    prev_desc = prev.get("desc")
    if isinstance(prev_desc, dict):
        has_existing = any(v for v in prev_desc.values() if v)
        if has_existing:
            desc = {}
            for lang in LANGS:
                if lang in prev_desc and prev_desc[lang]:
                    desc[lang] = prev_desc[lang]
                else:
                    # Fill missing language with Russian or first available
                    desc[lang] = prev_desc.get("ru") or next((v for v in prev_desc.values() if v), "")
        elif is_unverified and country_id:
            # Auto-generate enhanced description for unverified trails
            trail_type = determine_trail_type(gpx_path, stats, difficulty) if gpx_path else "xc"
            desc_text = generate_description(gpx_path, stats, difficulty, country_id, trail_type)
            desc = auto_translate_i18n(desc_text, "ru")
        else:
            desc = default_i18n("")
    elif is_unverified and country_id:
        # Auto-generate enhanced description for unverified trails
        trail_type = determine_trail_type(gpx_path, stats, difficulty) if gpx_path else "xc"
        desc_text = generate_description(gpx_path, stats, difficulty, country_id, trail_type)
        desc = auto_translate_i18n(desc_text, "ru")
    else:
        desc = default_i18n("")

    # Build in stable key order
    obj = {
        "id": tid,
        "cityId": city_id,
        "styles": styles,
        "name": name,
        "suitable": suitable,
        "difficulty": difficulty,
        "desc": desc,
        "gpxUrl": gpx_url,
        "startLat": start_lat,
        "startLon": start_lon,
    }
    
    # Add countryId if provided or if it exists in prev
    if country_id:
        obj["countryId"] = country_id
    elif "countryId" in prev:
        obj["countryId"] = prev["countryId"]

    # Preserve any extra custom fields that might exist in prev
    owned = set(obj.keys())
    if isinstance(prev, dict):
        for k, v in prev.items():
            if k not in owned:
                obj[k] = v

    return obj

def upsert_file(gpx_dir: Path, json_path: Path, gpx_folder_name: str, raw_base: str, 
                default_version: int, is_unverified: bool = False):
    """
    Ensures JSON is:
    {
      "version": <int>,
      "updatedAt": "YYYY-MM-DD",
      "trails": [ ... ]
    }
    
    For unverified trails, also scans country subfolders.
    """
    existing_root = load_root(json_path)

    # Read existing schema or create new
    if isinstance(existing_root, dict) and isinstance(existing_root.get("trails"), list):
        version = existing_root.get("version", default_version)
        updated_at = existing_root.get("updatedAt", today_yyyy_mm_dd())
        existing_trails = existing_root.get("trails", [])
    else:
        version = default_version
        updated_at = today_yyyy_mm_dd()
        existing_trails = []

    existing_by_id = {}
    order = []
    for t in existing_trails:
        if isinstance(t, dict) and t.get("id"):
            tid = t["id"]
            existing_by_id[tid] = t
            order.append(tid)

    # Collect all GPX files (root folder + country subfolders for unverified)
    all_gpx_files = []
    
    # Root folder GPX files
    if gpx_dir.exists():
        root_gpx = sorted([p for p in gpx_dir.glob("*.gpx") if p.is_file()])
        all_gpx_files.extend([(p, None) for p in root_gpx])  # (path, country_id)
    
    # Country subfolder GPX files (only for unverified)
    if is_unverified:
        for folder_name, country_code in COUNTRY_FOLDERS.items():
            folder_path = gpx_dir / folder_name
            if folder_path.exists():
                subfolder_gpx = sorted([p for p in folder_path.glob("*.gpx") if p.is_file()])
                all_gpx_files.extend([(p, country_code) for p in subfolder_gpx])
    
    file_by_id = {}
    for p, country_code in all_gpx_files:
        tid = p.stem
        file_by_id[tid] = (p, country_code)

    new_trails = []

    # Keep old order first
    for tid in order:
        if tid not in file_by_id:
            continue  # GPX removed -> remove from JSON
        
        p, country_code = file_by_id[tid]
        pts = parse_gpx_points(p, include_elevation=False)  # For backward compatibility

        # start coords from GPX if possible, else keep previous
        if pts:
            start_lat, start_lon = pts[0][0], pts[0][1]
        else:
            start_lat = existing_by_id[tid].get("startLat")
            start_lon = existing_by_id[tid].get("startLon")

        # Construct GPX URL based on location
        if country_code:
            folder_name = COUNTRY_CODE_TO_FOLDER.get(country_code, f"gpx_{country_code}")
            gpx_url = f"{raw_base}/{gpx_folder_name}/{folder_name}/{p.name}"
        else:
            gpx_url = f"{raw_base}/{gpx_folder_name}/{p.name}"
        
        new_trails.append(build_trail_object(
            existing_by_id[tid], tid, gpx_url, start_lat, start_lon,
            gpx_path=p, country_id=country_code, is_unverified=is_unverified
        ))

    # Add new GPX not present in JSON, sorted by id
    for tid in sorted(file_by_id.keys()):
        if tid in existing_by_id:
            continue
        
        p, country_code = file_by_id[tid]
        pts = parse_gpx_points(p, include_elevation=False)
        start_lat = pts[0][0] if pts else None
        start_lon = pts[0][1] if pts else None
        
        # Construct GPX URL based on location
        if country_code:
            folder_name = COUNTRY_CODE_TO_FOLDER.get(country_code, f"gpx_{country_code}")
            gpx_url = f"{raw_base}/{gpx_folder_name}/{folder_name}/{p.name}"
        else:
            gpx_url = f"{raw_base}/{gpx_folder_name}/{p.name}"
        
        new_trails.append(build_trail_object(
            {}, tid, gpx_url, start_lat, start_lon,
            gpx_path=p, country_id=country_code, is_unverified=is_unverified
        ))

    # EXTRA SAFETY: ensure cityId exists in every trail (even if something strange happens)
    for t in new_trails:
        if isinstance(t, dict):
            if "cityId" not in t:
                t["cityId"] = "chisinau"

    new_root = {
        "version": version,
        "updatedAt": updated_at,
        "trails": new_trails
    }

    # Update updatedAt only if trails changed (ignore updatedAt in comparison)
    if isinstance(existing_root, dict) and isinstance(existing_root.get("trails"), list):
        old_compare = dict(existing_root)
        new_compare = dict(new_root)
        old_compare["updatedAt"] = "X"
        new_compare["updatedAt"] = "X"

        if not roots_equal(old_compare, new_compare):
            new_root["updatedAt"] = today_yyyy_mm_dd()
        else:
            new_root["updatedAt"] = existing_root.get("updatedAt", updated_at)

    save_root(json_path, new_root)

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--verified-gpx-dir", required=True)
    ap.add_argument("--verified-json", required=True)
    ap.add_argument("--unverified-gpx-dir", required=True)
    ap.add_argument("--unverified-json", required=True)
    ap.add_argument("--raw-base", required=True)
    args = ap.parse_args()

    # Pre-processing: Split multi-track GPX files in unverified country folders
    print("Pre-processing: Checking for multi-track GPX files...")
    preprocess_split_multi_track_gpx(Path(args.unverified_gpx_dir))
    
    # Main processing
    print("\nProcessing verified trails...")
    upsert_file(Path(args.verified_gpx_dir), Path(args.verified_json), "gpx", args.raw_base, 
                default_version=10, is_unverified=False)
    
    print("\nProcessing unverified trails...")
    upsert_file(Path(args.unverified_gpx_dir), Path(args.unverified_json), "gpx_unverified", 
                args.raw_base, default_version=1, is_unverified=True)
    
    print("\nDone!")

if __name__ == "__main__":
    main()
